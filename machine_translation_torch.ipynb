{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wizard339/DLSchool/blob/main/machine_translation_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P13OapCzIwT"
      },
      "source": [
        "## Постановка задачи"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-NCaT9b-lQA"
      },
      "source": [
        "В рамках данного финального задания мы посторим сеть для перевода с немецкого языка на английский."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlPQCXQf9zwT"
      },
      "source": [
        "*Этот блокнот основан на [реализации с открытым исходным кодом](https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb) модели seq2seq NMT в PyTorch.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ9Lx1I3-ybs"
      },
      "source": [
        "Мы попробуем реализовать модель из этой статьи -> [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215).\n",
        "\n",
        "Модель будет обучена переводу с немецкого на английский, но ее можно применить к любой проблеме, связанной с переходом от одной последовательности к другой, например, к обобщению."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZFfVYKu_KfH"
      },
      "source": [
        "Наиболее распространенными моделями последовательностей (seq2seq) являются модели *энкодер-декодер*, которые (обычно) используют *рекуррентную нейронную сеть* (RNN) для *кодирования* исходного (входного) предложения в один вектор. В этом ноутбуке мы будем называть этот единственный вектор *контекстным вектором*. Вы можете рассматривать вектор контекста как абстрактное представление всего входного предложения. Этот вектор затем *декодируется* вторым RNN, который учится выводить целевое (выходное) предложение, генерируя его по одному слову за раз."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GwZM6Ep_XUZ"
      },
      "source": [
        "![](https://github.com/neychev/made_nlp_course/blob/master/week06_SelfAttention_and_NMT_practice/img/seq2seq1.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 torchtext==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7r_-K5xamDp",
        "outputId": "ea89e8c6-c5b9-492b-bcde-c561ac3e85d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.0%2Bcu111-cp38-cp38-linux_x86_64.whl (1982.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m904.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.9.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.9.0%2Bcu111-cp38-cp38-linux_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.8.0\n",
            "  Downloading torchaudio-0.8.0-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp38-cp38-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch==1.8.0+cu111) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.0+cu111) (4.4.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.9.0+cu111) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext==0.9.0) (2.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext==0.9.0) (4.64.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (2.10)\n",
            "Installing collected packages: torch, torchvision, torchtext, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.1+cu116\n",
            "    Uninstalling torchvision-0.14.1+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.1+cu116\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.14.1\n",
            "    Uninstalling torchtext-0.14.1:\n",
            "      Successfully uninstalled torchtext-0.14.1\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.13.1+cu116\n",
            "    Uninstalling torchaudio-0.13.1+cu116:\n",
            "      Successfully uninstalled torchaudio-0.13.1+cu116\n",
            "Successfully installed torch-1.8.0+cu111 torchaudio-0.8.0 torchtext-0.9.0 torchvision-0.9.0+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-ZlxYkLC20f"
      },
      "source": [
        "!python -m spacy download en > /dev/null\n",
        "!python -m spacy download de > /dev/null"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7140amzT9qCM"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "from torchtext.vocab import Vocab\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import io\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import spacy\n",
        "\n",
        "from collections import Counter\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mII7kLtZ40Nz"
      },
      "source": [
        "Зафиксируем все возможные случайные сиды, для максимальной воспроизводимости"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmlStWex_pvX"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgBRyR5nBflE"
      },
      "source": [
        "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
        "train_urls = ('train.de.gz', 'train.en.gz')\n",
        "val_urls = ('val.de.gz', 'val.en.gz')\n",
        "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RVdE7fNBi34"
      },
      "source": [
        "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
        "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
        "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "droT9QIi4_Ss"
      },
      "source": [
        "Проинициализируем токенизаторы для исходного и целевого языка:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY-2-Ph0Cybp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e909229-c92e-4489-c846-ece1592833ce"
      },
      "source": [
        "de_tokenizer = get_tokenizer('spacy', language='de')\n",
        "en_tokenizer = get_tokenizer('spacy', language='en')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"de\" could not be loaded, trying \"de_core_news_sm\" instead\n",
            "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n",
            "/usr/local/lib/python3.8/dist-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my53nYHOBwFI"
      },
      "source": [
        "## Загрузка данных\n",
        "\n",
        "Напишите функцию по созданию словаря, возвращающую словарь из `torchtext`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOm4WygbCLHI"
      },
      "source": [
        "def build_vocab(filepath, tokenizer):\n",
        "  # YOUR CODE HERE\n",
        "  with open(filepath) as f:\n",
        "    f_ = f.read()\n",
        "\n",
        "  tokens = tokenizer(f_)\n",
        "  counter = Counter(tokens)\n",
        "\n",
        "  return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCb6xpkiCM0l"
      },
      "source": [
        "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
        "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyQF0RwPDbzb"
      },
      "source": [
        "Проверка функции:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkD87GFyEoAX"
      },
      "source": [
        "assert en_vocab['a'] < 10\n",
        "assert de_vocab['Ein'] < 15000"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K30E1uWT5PgQ"
      },
      "source": [
        "Напишем функцию для составления большого тензора, который будет хранить на каждой строчке исходное токенизированное предложение и целевое."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Cwm-wy3DbKi"
      },
      "source": [
        "def data_process(filepaths):\n",
        "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "  data = []\n",
        "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
        "    de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)],\n",
        "                            dtype=torch.long)\n",
        "    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)],\n",
        "                            dtype=torch.long)\n",
        "    data.append((de_tensor_, en_tensor_))\n",
        "  return data"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sV7nGzsFMSC"
      },
      "source": [
        "Проинициализируйте переменные `train_data, val_data, test_data`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWhM0WZNFdXJ"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpToqCEyFjwd"
      },
      "source": [
        "assert len(train_data) == 29000\n",
        "assert len(val_data) == 1014\n",
        "assert len(test_data) == 1000\n",
        "assert train_data[23][0].sum().item() > 3220"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35zSx-HlGfhn"
      },
      "source": [
        "#YOUR CODE HERE\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jY4bt_CGkj0"
      },
      "source": [
        "assert device is not None\n",
        "assert device.type == 'cuda'"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Btfg-D1aGyQy"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "PAD_IDX = de_vocab['<pad>']\n",
        "BOS_IDX = de_vocab['<bos>']\n",
        "EOS_IDX = de_vocab['<eos>']"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VoleUx4G1G1"
      },
      "source": [
        "Дополните функцию `generate_batch` чтобы она добавляла паддинг в конце и чтобы первыми символами были `BOS_IDX`, а последним `EOS_IDX`."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cP3ZyY5is0KI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bkwmSw6HYL-"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "def generate_batch(data_batch):\n",
        "  de_batch, en_batch = [], []\n",
        "  max_len = max([max(len(de_item), len(en_item)) for de_item, en_item in data_batch])\n",
        "\n",
        "  for (de_item, en_item) in data_batch:\n",
        "    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor((max_len - len(de_item)) * [PAD_IDX]), torch.tensor([EOS_IDX])], dim=0))\n",
        "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor((max_len - len(en_item)) * [PAD_IDX]), torch.tensor([EOS_IDX])], dim=0))\n",
        "\n",
        "  de_batch = torch.stack(de_batch)\n",
        "  en_batch = torch.stack(en_batch)\n",
        "\n",
        "  return de_batch, en_batch"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QABYiLrGJFf"
      },
      "source": [
        "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                       shuffle=True, collate_fn=generate_batch)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F9WdPUPHuDO"
      },
      "source": [
        "import random\n",
        "from typing import Tuple\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9CZqwEm8kgR"
      },
      "source": [
        "## Подготовка модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ5FSyXU503R"
      },
      "source": [
        "Наконец мы переходим к написанию самой нейросети. Она будет состоять из 3 различных классов, а финальная сеть будет состоять из этих трех классов. Получится несколько объемно, но когда мы начинаем переходить к более серьезным сетям."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNK-ORW1In6y"
      },
      "source": [
        "Создадим модель Seq2Seq на основе GRU:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JekezzB5I0_O"
      },
      "source": [
        "Создайте класс `Encoder`, который состоит из слоя Embedding, GRU(bidirectional) и Linear:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvvpXnXcIxNT"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim: int,\n",
        "                 emb_dim: int,\n",
        "                 enc_hid_dim: int,\n",
        "                 dec_hid_dim: int,\n",
        "                 dropout: float):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Your code here\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor) -> Tuple[Tensor]:\n",
        "\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:],\n",
        "                                               hidden[-1,:,:]),\n",
        "                                               dim = 1)))\n",
        "\n",
        "        return outputs, hidden"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9QCQDhkJUZy"
      },
      "source": [
        "Оставим класс Attention, чтобы сохранить преемственность с оригинальной статьей. Подбробнее про Attention мы поговорим в следующих модулях."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRCJw2w1Iv5Y"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 enc_hid_dim: int,\n",
        "                 dec_hid_dim: int,\n",
        "                 attn_dim: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "\n",
        "        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n",
        "\n",
        "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
        "\n",
        "    def forward(self,\n",
        "                decoder_hidden: Tensor,\n",
        "                encoder_outputs: Tensor) -> Tensor:\n",
        "\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "\n",
        "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        energy = torch.tanh(self.attn(torch.cat((\n",
        "            repeated_decoder_hidden,\n",
        "            encoder_outputs),\n",
        "            dim = 2)))\n",
        "\n",
        "        attention = torch.sum(energy, dim=2)\n",
        "\n",
        "        return F.softmax(attention, dim=1)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RdFhl6KJvB9"
      },
      "source": [
        "Добавим класс `Decoder`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7WCAa7YJyg2"
      },
      "source": [
        "Аналогично классу `Encoder` сделайте класс `Decoder` таким же по архитектуре, однако линейный слой будет иметь размерность emb_dim  плюс размерность выходного слоя Attention."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk-tKFwoKCIp"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 output_dim: int,\n",
        "                 emb_dim: int,\n",
        "                 enc_hid_dim: int,\n",
        "                 dec_hid_dim: int,\n",
        "                 dropout: int,\n",
        "                 attention: nn.Module):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout = dropout\n",
        "        self.attention = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        \n",
        "        self.out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def _weighted_encoder_rep(self,\n",
        "                              decoder_hidden: Tensor,\n",
        "                              encoder_outputs: Tensor) -> Tensor:\n",
        "\n",
        "        a = self.attention(decoder_hidden, encoder_outputs)\n",
        "\n",
        "        a = a.unsqueeze(1)\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
        "\n",
        "        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n",
        "\n",
        "        return weighted_encoder_rep\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                input: Tensor,\n",
        "                decoder_hidden: Tensor,\n",
        "                encoder_outputs: Tensor) -> Tuple[Tensor]:\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n",
        "                                                          encoder_outputs)\n",
        "\n",
        "        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n",
        "\n",
        "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
        "\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n",
        "\n",
        "        output = self.out(torch.cat((output,\n",
        "                                     weighted_encoder_rep,\n",
        "                                     embedded), dim = 1))\n",
        "\n",
        "        return output, decoder_hidden.squeeze(0)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "steO_DGUKxu_"
      },
      "source": [
        "Наконец, объединим все в класс модели `Seq2Seq`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8yAxyZiK4t4"
      },
      "source": [
        "Напишите, что должен принимать на вход класс Seq2Seq и проинициализируйте экземпляр этого класса:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ict3MDeKxOj"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,\n",
        "                 encoder: nn.Module,\n",
        "                 decoder: nn.Module,\n",
        "                 device: torch.device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "        assert encoder.enc_hid_dim == decoder.enc_hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.dec_hid_dim == decoder.dec_hid_dim, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                teacher_forcing_ratio: float = 0.5) -> Tensor:\n",
        "\n",
        "        batch_size = src.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        # first input to the decoder is the <sos> token\n",
        "        output = trg[0,:]\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.max(1)[1]\n",
        "            output = (trg[t] if teacher_force else top1)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-FbdHsi6uFC"
      },
      "source": [
        "Вы можете попробовать тут свои значения для разных слоев. Единственное - правильно задайте размеры входных слоев для Encoder и Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXUquwEVLI4m"
      },
      "source": [
        "INPUT_DIM = len(de_vocab)\n",
        "OUTPUT_DIM = len(en_vocab)\n",
        "ENC_EMB_DIM = 64\n",
        "DEC_EMB_DIM = 64\n",
        "ENC_HID_DIM = 128\n",
        "DEC_HID_DIM = 128\n",
        "ATTN_DIM = 32\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "att = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, att)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3bwd5mWLnMC"
      },
      "source": [
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sxA2jKgLxu2"
      },
      "source": [
        "Напишите функцию, считающую параметры модели:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khvc3_qgL8P4"
      },
      "source": [
        "def count_parameters(model: nn.Module):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR5jUX5LMEQt"
      },
      "source": [
        "assert count_parameters(model) > 7_000_000"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2opeAdkHcmxn",
        "outputId": "e358bea9-d037-4115-ea9a-d2843d03e494"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7156662"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ots0k0mMMSJo"
      },
      "source": [
        "Проинициализируем веса, как это рекомендуют в оригинальной статье:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs8cWMvnJdSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be309b2f-1229-40ef-cb29-df62e3b603e0"
      },
      "source": [
        "def init_weights(m: nn.Module):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(19215, 64)\n",
              "    (rnn): GRU(64, 128, bidirectional=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (fc): Linear(in_features=256, out_features=128, bias=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=384, out_features=32, bias=True)\n",
              "    )\n",
              "    (embedding): Embedding(10838, 64)\n",
              "    (rnn): GRU(320, 128)\n",
              "    (out): Linear(in_features=448, out_features=10838, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-wKKaoDJdMY"
      },
      "source": [
        "PAD_IDX = en_vocab.stoi['<pad>']\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfBl6-DF8yUZ"
      },
      "source": [
        "## Обучение модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzSjMJYffi21"
      },
      "source": [
        "Напишите функцию обучения модели. Если получится, добавьте в тренировочный цикл запись значений функции потерь и перплексии в tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ-0BmNgf1YA"
      },
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "\n",
        "def train(model: nn.Module,\n",
        "          iterator: torch.utils.data.DataLoader,\n",
        "          optimizer: optim.Optimizer,\n",
        "          criterion: nn.Module,\n",
        "          clip: float, \n",
        "          writer=None):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for _, (src, trg) in enumerate(iterator):\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPC0UqNO7PSE"
      },
      "source": [
        "Напишем функцию для валидации модели. Хочу отметить, что при валидации наиболее важным моментом является выключение метода teacher-forcing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs7FWIJ-MgfW"
      },
      "source": [
        "def evaluate(model: nn.Module,\n",
        "             iterator: torch.utils.data.DataLoader,\n",
        "             criterion: nn.Module):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _, (src, trg) in enumerate(iterator):\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICUCceM6M047"
      },
      "source": [
        "def epoch_time(start_time: int,\n",
        "               end_time: int):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjmAW7RiM0xW"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iter, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "test_loss = evaluate(model, test_iter, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([torch.tensor([PAD_IDX]), torch.tensor([PAD_IDX])]).dim()"
      ],
      "metadata": {
        "id": "8BNXoMqume1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MPC8pElfI-2"
      },
      "source": [
        "Добейтесь перплексии на валидационной выборке порядка 35-40. Попробуйте различные значения параметров сети и learning rate."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hns9vI_d0MmT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}